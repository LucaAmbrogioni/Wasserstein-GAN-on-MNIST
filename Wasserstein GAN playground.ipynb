{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/optdcc/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "train, test = datasets.get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    batch_indices = np.random.choice(range(len(train)), size=(batch_size,), replace=False)\n",
    "    return np.array([np.reshape(train[k][0], newshape=(1,28,28)) for k in batch_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_input(batch_size, n_hidden):\n",
    "    return np.random.normal(0,1,size=(batch_size, n_hidden)).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(generator, n_images=5):\n",
    "    # Generate Images\n",
    "    z = get_random_input(n_hidden=generator.n_hidden, batch_size=n_images)\n",
    "    images = generator(z).data\n",
    "    \n",
    "    #Plot Images\n",
    "    f, axis = plt.subplots(1, n_images, sharey=True)\n",
    "    for index, ax in enumerate(axis):\n",
    "        ax.imshow(images[index,0,:,:])\n",
    "    plt.title(\"Generated Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(chainer.Chain):\n",
    "    def __init__(self, n_hidden, bottom_width=3, ch=512, wscale=0.02):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.ch = ch\n",
    "        self.bottom_width = bottom_width\n",
    "\n",
    "        with self.init_scope():\n",
    "            w = chainer.initializers.Normal(wscale)\n",
    "            self.l0 = L.Linear(in_size=n_hidden, out_size=bottom_width*bottom_width*ch, initialW=w)\n",
    "\n",
    "            self.dc1 = L.Deconvolution2D(in_channels=ch, out_channels=ch//2, ksize=2, stride=2, pad=1, initialW=w)\n",
    "            self.dc2 = L.Deconvolution2D(in_channels=ch//2, out_channels=ch//4, ksize=2, stride=2, pad=1, initialW=w)\n",
    "            self.dc3 = L.Deconvolution2D(in_channels=ch//4, out_channels=ch//8, ksize=2, stride=2, pad=1, initialW=w)\n",
    "            self.dc4 = L.Deconvolution2D(in_channels=ch//8, out_channels=1, ksize=3, stride=3, pad=1, initialW=w)\n",
    "\n",
    "            # self.bn0 = L.BatchNormalization(size=self.bottom_width*self.bottom_width*self.ch)\n",
    "            self.bn1 = L.BatchNormalization(size=ch)\n",
    "            self.bn2 = L.BatchNormalization(size=ch//2)\n",
    "            self.bn3 = L.BatchNormalization(size=ch//4)\n",
    "            self.bn4 = L.BatchNormalization(size=ch//8)\n",
    "\n",
    "    def make_hidden(self, batchsize):\n",
    "        return numpy.random.uniform(-1, 1, (batchsize, self.n_hidden, 1, 1)).astype(numpy.float32)\n",
    "\n",
    "    def __call__(self, z):\n",
    "        h = self.l0(z)\n",
    "        h = F.reshape(h, (len(z), self.ch, self.bottom_width, self.bottom_width))\n",
    "        h = F.relu(self.bn1(h))\n",
    "        h = F.relu(self.bn2(self.dc1(h)))\n",
    "        h = F.relu(self.bn3(self.dc2(h)))\n",
    "        h = F.relu(self.bn4(self.dc3(h)))\n",
    "        x = F.sigmoid(self.dc4(h))\n",
    "        # x = F.tanh(self.dc4(h))\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Discriminator(chainer.Chain):\n",
    "    def __init__(self, bottom_width=3, ch=512, wscale=0.02):\n",
    "        w = chainer.initializers.Normal(wscale)\n",
    "        super(Discriminator, self).__init__()\n",
    "        with self.init_scope():\n",
    "\n",
    "            self.c0 = L.Convolution2D(in_channels=1, out_channels=64, ksize=3, stride=3, pad=1, initialW=w)\n",
    "            self.c1 = L.Convolution2D(in_channels=ch//8, out_channels=128, ksize=2, stride=2, pad=1, initialW=w)\n",
    "            self.c2 = L.Convolution2D(in_channels=ch//4, out_channels=256, ksize=2, stride=2, pad=1, initialW=w)\n",
    "            self.c3 = L.Convolution2D(in_channels=ch//2, out_channels=512, ksize=2, stride=2, pad=1, initialW=w)\n",
    "\n",
    "            # self.l4 = L.Linear(in_size=bottom_width*bottom_width*ch, out_size=1, initialW=w)\n",
    "            self.l4 = L.Linear(in_size=None, out_size=1, initialW=w)\n",
    "\n",
    "            # self.bn0 = L.BatchNormalization(size=ch//8, use_gamma=False)\n",
    "            self.bn1 = L.BatchNormalization(size=ch//4, use_gamma=False)\n",
    "            self.bn2 = L.BatchNormalization(size=ch//2, use_gamma=False)\n",
    "            self.bn3 = L.BatchNormalization(size=ch//1, use_gamma=False)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.leaky_relu(self.c0(x))\n",
    "        h = F.leaky_relu(self.bn1(self.c1(h)))\n",
    "        h = F.leaky_relu(self.bn2(self.c2(h)))\n",
    "        h = F.leaky_relu(self.bn3(self.c3(h)))\n",
    "        y = self.l4(h)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wGAN(n_iter, batch_size, generator, discriminator, regularization = 0.):\n",
    "    \n",
    "    # Setup Optimizers\n",
    "    gen_optimizer = optimizers.Adam(alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-08)\n",
    "    gen_optimizer.setup(generator)\n",
    "    dis_optimizer = optimizers.Adam(alpha=0.00001, beta1=0.9, beta2=0.999, eps=1e-08)\n",
    "    dis_optimizer.setup(discriminator)\n",
    "    \n",
    "    #Train\n",
    "    for itr in range(n_iter):\n",
    "        #Clear gradients\n",
    "        generator.cleargrads()\n",
    "        discriminator.cleargrads()\n",
    "        \n",
    "        # Sample real batch\n",
    "        real_batch = get_batch(batch_size)\n",
    "        \n",
    "        # Generate batch\n",
    "        latent = get_random_input(batch_size, generator.n_hidden)\n",
    "        generated_batch = generator(latent)\n",
    "        \n",
    "        # Compute loss\n",
    "        gen_loss = wGAN_loss(discriminator, generated_batch, real_batch, squared=False)\n",
    "        \n",
    "        # Update Generator\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.update()\n",
    "        \n",
    "        # Update discriminator\n",
    "        discriminator.cleargrads()\n",
    "        reg_loss = wGAN_regularization(discriminator, real_batch)\n",
    "        (-gen_loss + regularization*reg_loss).backward()\n",
    "        dis_optimizer.update()\n",
    "        \n",
    "        # Plot Images\n",
    "        print(\"Iteration: {}, loss: {}, reg: {}\".format(itr,gen_loss.data,reg_loss.data))\n",
    "        if itr%100 == 0:\n",
    "            plot_generated_images(generator, n_images=5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wGAN_loss(discriminator, generated_batch, train_batch, squared=False):\n",
    "    gen_loss = F.mean(discriminator(generated_batch)) - F.mean(discriminator(train_batch))\n",
    "    if squared:\n",
    "        gen_loss = gen_loss**2\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wGAN_regularization(discriminator, train_batch):\n",
    "    batch_size = np.shape(train_batch)[0]\n",
    "    half_batch_1 = train_batch[:batch_size/2,:,:,:]\n",
    "    half_batch_2 = train_batch[batch_size/2:,:,:,:]\n",
    "    in_difference = F.sqrt(F.sum((half_batch_1 - half_batch_2)**2,axis=(1,2,3)))\n",
    "    out_difference = F.sum(F.absolute(discriminator(half_batch_1) - discriminator(half_batch_2)), axis=1)\n",
    "    loss = F.mean(F.relu(out_difference - in_difference)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 50\n",
    "generator = Generator(n_hidden=n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss: 6.83500146866, reg: 0.0\n",
      "Iteration: 1, loss: 6.52949810028, reg: 0.0\n",
      "Iteration: 2, loss: 6.48291730881, reg: 0.0\n",
      "Iteration: 3, loss: 6.57677459717, reg: 0.0\n",
      "Iteration: 4, loss: 6.20837831497, reg: 0.0\n",
      "Iteration: 5, loss: 6.09701824188, reg: 0.0\n",
      "Iteration: 6, loss: 5.94340658188, reg: 0.0\n",
      "Iteration: 7, loss: 6.07276725769, reg: 0.0\n",
      "Iteration: 8, loss: 6.03432846069, reg: 0.0\n",
      "Iteration: 9, loss: 6.32420921326, reg: 0.0\n",
      "Iteration: 10, loss: 6.46693897247, reg: 0.0\n",
      "Iteration: 11, loss: 6.47410011292, reg: 0.0\n",
      "Iteration: 12, loss: 6.38352966309, reg: 0.0\n",
      "Iteration: 13, loss: 6.07825183868, reg: 0.0\n",
      "Iteration: 14, loss: 6.13002252579, reg: 0.0\n",
      "Iteration: 15, loss: 6.16052246094, reg: 0.0\n",
      "Iteration: 16, loss: 6.1343870163, reg: 0.0\n",
      "Iteration: 17, loss: 6.15612125397, reg: 0.0\n",
      "Iteration: 18, loss: 5.73535919189, reg: 0.0\n",
      "Iteration: 19, loss: 6.07588005066, reg: 0.0\n",
      "Iteration: 20, loss: 6.02744150162, reg: 0.0\n",
      "Iteration: 21, loss: 5.88344240189, reg: 0.0\n",
      "Iteration: 22, loss: 5.81597280502, reg: 0.0\n",
      "Iteration: 23, loss: 6.04086875916, reg: 0.0\n",
      "Iteration: 24, loss: 6.29455947876, reg: 0.0\n",
      "Iteration: 25, loss: 5.83311462402, reg: 0.0\n",
      "Iteration: 26, loss: 6.08628988266, reg: 0.0\n",
      "Iteration: 27, loss: 6.03657913208, reg: 0.0\n",
      "Iteration: 28, loss: 6.21935892105, reg: 0.0\n",
      "Iteration: 29, loss: 5.68993425369, reg: 0.0\n",
      "Iteration: 30, loss: 5.96659803391, reg: 0.0\n",
      "Iteration: 31, loss: 5.79507541656, reg: 0.0\n",
      "Iteration: 32, loss: 5.89750671387, reg: 0.0\n",
      "Iteration: 33, loss: 5.64458751678, reg: 0.0\n",
      "Iteration: 34, loss: 5.87282133102, reg: 0.0\n",
      "Iteration: 35, loss: 5.76422834396, reg: 0.0\n",
      "Iteration: 36, loss: 5.53919219971, reg: 0.0\n",
      "Iteration: 37, loss: 5.65220451355, reg: 0.0\n",
      "Iteration: 38, loss: 5.19533872604, reg: 0.0\n",
      "Iteration: 39, loss: 4.94646167755, reg: 0.0\n",
      "Iteration: 40, loss: 4.95785045624, reg: 0.0\n",
      "Iteration: 41, loss: 4.86474466324, reg: 0.0\n",
      "Iteration: 42, loss: 4.71093559265, reg: 0.0\n",
      "Iteration: 43, loss: 4.88807249069, reg: 0.0\n",
      "Iteration: 44, loss: 4.86723566055, reg: 0.0\n",
      "Iteration: 45, loss: 4.4092259407, reg: 0.0\n",
      "Iteration: 46, loss: 4.66170978546, reg: 0.0\n",
      "Iteration: 47, loss: 4.77353382111, reg: 0.0\n",
      "Iteration: 48, loss: 4.43045854568, reg: 0.0\n",
      "Iteration: 49, loss: 4.31102752686, reg: 0.0\n",
      "Iteration: 50, loss: 4.47307825089, reg: 0.0\n",
      "Iteration: 51, loss: 4.42701864243, reg: 0.0\n",
      "Iteration: 52, loss: 4.78137493134, reg: 0.0\n",
      "Iteration: 53, loss: 4.51469802856, reg: 0.0\n",
      "Iteration: 54, loss: 4.46990203857, reg: 0.0\n",
      "Iteration: 55, loss: 4.55791044235, reg: 0.0\n",
      "Iteration: 56, loss: 4.60345125198, reg: 0.0\n",
      "Iteration: 57, loss: 4.73185825348, reg: 0.0\n",
      "Iteration: 58, loss: 5.08659124374, reg: 0.0\n",
      "Iteration: 59, loss: 5.02779626846, reg: 0.0\n",
      "Iteration: 60, loss: 5.30603313446, reg: 0.0\n",
      "Iteration: 61, loss: 4.96674251556, reg: 0.0\n",
      "Iteration: 62, loss: 5.27026414871, reg: 0.0\n",
      "Iteration: 63, loss: 5.04458761215, reg: 0.0\n",
      "Iteration: 64, loss: 4.93618154526, reg: 0.0\n",
      "Iteration: 65, loss: 5.2845454216, reg: 0.0\n",
      "Iteration: 66, loss: 4.87392139435, reg: 0.0\n",
      "Iteration: 67, loss: 4.91414785385, reg: 0.0\n",
      "Iteration: 68, loss: 4.58496379852, reg: 0.0\n",
      "Iteration: 69, loss: 4.77373933792, reg: 0.0\n",
      "Iteration: 70, loss: 4.35769557953, reg: 0.0\n",
      "Iteration: 71, loss: 4.37384700775, reg: 0.0\n",
      "Iteration: 72, loss: 4.20183944702, reg: 0.0\n",
      "Iteration: 73, loss: 3.97714304924, reg: 0.0\n",
      "Iteration: 74, loss: 4.10652828217, reg: 0.0\n",
      "Iteration: 75, loss: 3.80606603622, reg: 0.0\n",
      "Iteration: 76, loss: 3.82017850876, reg: 0.0\n",
      "Iteration: 77, loss: 4.11561870575, reg: 0.0\n",
      "Iteration: 78, loss: 4.21040773392, reg: 0.0\n",
      "Iteration: 79, loss: 4.108481884, reg: 0.0\n",
      "Iteration: 80, loss: 4.27929735184, reg: 0.0\n",
      "Iteration: 81, loss: 4.14293432236, reg: 0.0\n",
      "Iteration: 82, loss: 4.50537347794, reg: 0.0\n",
      "Iteration: 83, loss: 4.5331864357, reg: 0.0\n",
      "Iteration: 84, loss: 4.25620508194, reg: 0.0\n",
      "Iteration: 85, loss: 4.36689424515, reg: 0.0\n",
      "Iteration: 86, loss: 4.47742891312, reg: 0.0\n",
      "Iteration: 87, loss: 4.15792417526, reg: 0.0\n",
      "Iteration: 88, loss: 4.38702344894, reg: 0.0\n",
      "Iteration: 89, loss: 4.12003946304, reg: 0.0\n",
      "Iteration: 90, loss: 3.89734220505, reg: 0.0\n",
      "Iteration: 91, loss: 3.78106021881, reg: 0.0\n",
      "Iteration: 92, loss: 3.65634632111, reg: 0.0\n",
      "Iteration: 93, loss: 3.52054786682, reg: 0.0\n",
      "Iteration: 94, loss: 3.35355091095, reg: 0.0\n",
      "Iteration: 95, loss: 2.99517631531, reg: 0.0\n",
      "Iteration: 96, loss: 2.75948238373, reg: 0.0\n",
      "Iteration: 97, loss: 2.85376739502, reg: 0.0\n",
      "Iteration: 98, loss: 2.52260971069, reg: 0.0\n",
      "Iteration: 99, loss: 2.44960451126, reg: 0.0\n",
      "Iteration: 100, loss: 2.14097738266, reg: 0.0\n",
      "Iteration: 101, loss: 2.40574336052, reg: 0.0\n",
      "Iteration: 102, loss: 2.4127342701, reg: 0.0\n",
      "Iteration: 103, loss: 2.44939184189, reg: 0.0\n",
      "Iteration: 104, loss: 2.21410751343, reg: 0.0\n",
      "Iteration: 105, loss: 2.18614983559, reg: 0.0\n",
      "Iteration: 106, loss: 2.37018585205, reg: 0.0\n",
      "Iteration: 107, loss: 2.22949838638, reg: 0.0\n",
      "Iteration: 108, loss: 2.51961970329, reg: 0.0\n",
      "Iteration: 109, loss: 2.483066082, reg: 0.0\n",
      "Iteration: 110, loss: 2.39524126053, reg: 0.0\n",
      "Iteration: 111, loss: 2.14449882507, reg: 0.0\n",
      "Iteration: 112, loss: 2.28107118607, reg: 0.0\n",
      "Iteration: 113, loss: 2.62031316757, reg: 0.0\n",
      "Iteration: 114, loss: 2.39821577072, reg: 0.0\n",
      "Iteration: 115, loss: 2.33076143265, reg: 0.0\n",
      "Iteration: 116, loss: 2.55307865143, reg: 0.0\n",
      "Iteration: 117, loss: 2.08911776543, reg: 0.0\n",
      "Iteration: 118, loss: 2.18773245811, reg: 0.0\n",
      "Iteration: 119, loss: 1.942694664, reg: 0.0\n",
      "Iteration: 120, loss: 2.1443874836, reg: 0.0\n",
      "Iteration: 121, loss: 1.91304278374, reg: 0.0\n",
      "Iteration: 122, loss: 1.65182948112, reg: 0.0\n",
      "Iteration: 123, loss: 1.6282658577, reg: 0.0\n",
      "Iteration: 124, loss: 1.39677095413, reg: 0.0\n",
      "Iteration: 125, loss: 1.34173834324, reg: 0.0\n",
      "Iteration: 126, loss: 1.25250422955, reg: 0.0\n",
      "Iteration: 127, loss: 1.32948279381, reg: 0.0\n",
      "Iteration: 128, loss: 1.46062612534, reg: 0.0\n",
      "Iteration: 129, loss: 1.59164881706, reg: 0.0\n",
      "Iteration: 130, loss: 1.46239423752, reg: 0.0\n",
      "Iteration: 131, loss: 1.52401292324, reg: 0.0\n",
      "Iteration: 132, loss: 1.36960268021, reg: 0.0\n",
      "Iteration: 133, loss: 1.55688643456, reg: 0.0\n",
      "Iteration: 134, loss: 1.52052593231, reg: 0.0\n",
      "Iteration: 135, loss: 1.40002322197, reg: 0.0\n",
      "Iteration: 136, loss: 1.17964029312, reg: 0.0\n",
      "Iteration: 137, loss: 1.51927661896, reg: 0.0\n",
      "Iteration: 138, loss: 1.7339758873, reg: 0.0\n",
      "Iteration: 139, loss: 1.73296117783, reg: 0.0\n",
      "Iteration: 140, loss: 1.30640161037, reg: 0.0\n",
      "Iteration: 141, loss: 1.74312579632, reg: 0.0\n",
      "Iteration: 142, loss: 1.63519787788, reg: 0.0\n",
      "Iteration: 143, loss: 1.48837041855, reg: 0.0\n",
      "Iteration: 144, loss: 1.90628194809, reg: 0.0\n",
      "Iteration: 145, loss: 1.74103713036, reg: 0.0\n",
      "Iteration: 146, loss: 1.83375477791, reg: 0.0\n",
      "Iteration: 147, loss: 1.50050473213, reg: 0.0\n",
      "Iteration: 148, loss: 1.5117944479, reg: 0.0\n",
      "Iteration: 149, loss: 1.93204510212, reg: 0.0\n",
      "Iteration: 150, loss: 1.90243411064, reg: 0.0\n",
      "Iteration: 151, loss: 1.59511327744, reg: 0.0\n",
      "Iteration: 152, loss: 1.5005800724, reg: 0.0\n",
      "Iteration: 153, loss: 1.74464416504, reg: 0.0\n",
      "Iteration: 154, loss: 1.91187334061, reg: 0.0\n",
      "Iteration: 155, loss: 1.7444807291, reg: 0.0\n",
      "Iteration: 156, loss: 1.70720922947, reg: 0.0\n",
      "Iteration: 157, loss: 1.77697443962, reg: 0.0\n",
      "Iteration: 158, loss: 1.95741868019, reg: 0.0\n",
      "Iteration: 159, loss: 1.95726823807, reg: 0.0\n",
      "Iteration: 160, loss: 1.98629283905, reg: 0.0\n",
      "Iteration: 161, loss: 1.72940456867, reg: 0.0\n",
      "Iteration: 162, loss: 1.90528130531, reg: 0.0\n",
      "Iteration: 163, loss: 2.02650332451, reg: 0.0\n",
      "Iteration: 164, loss: 1.97207689285, reg: 0.0\n",
      "Iteration: 165, loss: 2.10167074203, reg: 0.0\n",
      "Iteration: 166, loss: 2.34698963165, reg: 0.0\n",
      "Iteration: 167, loss: 1.9508023262, reg: 0.0\n",
      "Iteration: 168, loss: 2.36847019196, reg: 0.0\n",
      "Iteration: 169, loss: 2.32733821869, reg: 0.0\n",
      "Iteration: 170, loss: 2.38154745102, reg: 0.0\n",
      "Iteration: 171, loss: 2.33098173141, reg: 0.0\n",
      "Iteration: 172, loss: 2.0832028389, reg: 0.0\n",
      "Iteration: 173, loss: 2.64301991463, reg: 0.0\n",
      "Iteration: 174, loss: 2.22086596489, reg: 0.0\n",
      "Iteration: 175, loss: 2.42884874344, reg: 0.0\n",
      "Iteration: 176, loss: 2.49028778076, reg: 0.0\n",
      "Iteration: 177, loss: 2.50437784195, reg: 0.0\n",
      "Iteration: 178, loss: 2.57528853416, reg: 0.0\n",
      "Iteration: 179, loss: 2.5064637661, reg: 0.0\n",
      "Iteration: 180, loss: 2.87953281403, reg: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 181, loss: 2.4051835537, reg: 0.0\n",
      "Iteration: 182, loss: 2.54642319679, reg: 0.0\n",
      "Iteration: 183, loss: 2.64255428314, reg: 0.0\n",
      "Iteration: 184, loss: 2.64531207085, reg: 0.0\n",
      "Iteration: 185, loss: 2.65293931961, reg: 0.0\n",
      "Iteration: 186, loss: 2.1090862751, reg: 0.0\n",
      "Iteration: 187, loss: 2.55027961731, reg: 0.0\n",
      "Iteration: 188, loss: 2.40825653076, reg: 0.0\n",
      "Iteration: 189, loss: 2.46527600288, reg: 0.0\n",
      "Iteration: 190, loss: 1.99017941952, reg: 0.0\n",
      "Iteration: 191, loss: 1.76657676697, reg: 0.0\n",
      "Iteration: 192, loss: 2.16481685638, reg: 0.0\n",
      "Iteration: 193, loss: 1.93088388443, reg: 0.0\n",
      "Iteration: 194, loss: 1.58479690552, reg: 0.0\n",
      "Iteration: 195, loss: 1.73540735245, reg: 0.0\n",
      "Iteration: 196, loss: 1.43326330185, reg: 0.0\n",
      "Iteration: 197, loss: 1.4669380188, reg: 0.0\n",
      "Iteration: 198, loss: 1.50754916668, reg: 0.0\n",
      "Iteration: 199, loss: 1.43736195564, reg: 0.0\n",
      "Iteration: 200, loss: 1.29206109047, reg: 0.0\n",
      "Iteration: 201, loss: 1.20288598537, reg: 0.0\n",
      "Iteration: 202, loss: 0.842690944672, reg: 0.0\n",
      "Iteration: 203, loss: 0.796969056129, reg: 0.0\n",
      "Iteration: 204, loss: 0.7451608181, reg: 0.0\n",
      "Iteration: 205, loss: 1.03949415684, reg: 0.0\n",
      "Iteration: 206, loss: 1.06230664253, reg: 0.0\n",
      "Iteration: 207, loss: 0.878457963467, reg: 0.0\n",
      "Iteration: 208, loss: 1.01024925709, reg: 0.0\n",
      "Iteration: 209, loss: 1.01391363144, reg: 0.0\n",
      "Iteration: 210, loss: 1.12970113754, reg: 0.0\n",
      "Iteration: 211, loss: 0.765809297562, reg: 0.0\n",
      "Iteration: 212, loss: 0.905060172081, reg: 0.0\n",
      "Iteration: 213, loss: 0.763597726822, reg: 0.0\n",
      "Iteration: 214, loss: 0.959723830223, reg: 0.0\n",
      "Iteration: 215, loss: 1.19659352303, reg: 0.0\n",
      "Iteration: 216, loss: 0.856048345566, reg: 0.0\n",
      "Iteration: 217, loss: 1.27690267563, reg: 0.0\n",
      "Iteration: 218, loss: 1.0660841465, reg: 0.0\n",
      "Iteration: 219, loss: 0.990347325802, reg: 0.0\n",
      "Iteration: 220, loss: 0.852088093758, reg: 0.0\n",
      "Iteration: 221, loss: 0.929219782352, reg: 0.0\n",
      "Iteration: 222, loss: 1.19540214539, reg: 0.0\n",
      "Iteration: 223, loss: 1.14163756371, reg: 0.0\n",
      "Iteration: 224, loss: 1.08662283421, reg: 0.0\n",
      "Iteration: 225, loss: 1.41102099419, reg: 0.0\n",
      "Iteration: 226, loss: 1.39206266403, reg: 0.0\n",
      "Iteration: 227, loss: 1.19184184074, reg: 0.0\n",
      "Iteration: 228, loss: 1.24146866798, reg: 0.0\n",
      "Iteration: 229, loss: 1.43320083618, reg: 0.0\n",
      "Iteration: 230, loss: 1.19285190105, reg: 0.0\n",
      "Iteration: 231, loss: 1.10648798943, reg: 0.0\n",
      "Iteration: 232, loss: 1.31803870201, reg: 0.0\n",
      "Iteration: 233, loss: 1.23687291145, reg: 0.0\n",
      "Iteration: 234, loss: 1.28273391724, reg: 0.0\n",
      "Iteration: 235, loss: 1.10252141953, reg: 0.0\n",
      "Iteration: 236, loss: 1.36266160011, reg: 0.0\n",
      "Iteration: 237, loss: 1.26714873314, reg: 0.0\n",
      "Iteration: 238, loss: 1.23949444294, reg: 0.0\n",
      "Iteration: 239, loss: 1.3255212307, reg: 0.0\n",
      "Iteration: 240, loss: 1.13219892979, reg: 0.0\n",
      "Iteration: 241, loss: 1.32414674759, reg: 0.0\n",
      "Iteration: 242, loss: 1.138915658, reg: 0.0\n",
      "Iteration: 243, loss: 0.998591721058, reg: 0.0\n",
      "Iteration: 244, loss: 1.09461224079, reg: 0.0\n",
      "Iteration: 245, loss: 1.03060936928, reg: 0.0\n",
      "Iteration: 246, loss: 1.15526127815, reg: 0.0\n",
      "Iteration: 247, loss: 1.21845924854, reg: 0.0\n",
      "Iteration: 248, loss: 1.02120900154, reg: 0.0\n",
      "Iteration: 249, loss: 1.19683265686, reg: 0.0\n",
      "Iteration: 250, loss: 0.975348889828, reg: 0.0\n",
      "Iteration: 251, loss: 1.23385024071, reg: 0.0\n",
      "Iteration: 252, loss: 1.15026676655, reg: 0.0\n",
      "Iteration: 253, loss: 0.928436398506, reg: 0.0\n",
      "Iteration: 254, loss: 1.52845907211, reg: 0.0\n",
      "Iteration: 255, loss: 1.41129779816, reg: 0.0\n",
      "Iteration: 256, loss: 1.01594555378, reg: 0.0\n",
      "Iteration: 257, loss: 1.31409549713, reg: 0.0\n",
      "Iteration: 258, loss: 1.27850270271, reg: 0.0\n",
      "Iteration: 259, loss: 1.22159409523, reg: 0.0\n",
      "Iteration: 260, loss: 1.00181794167, reg: 0.0\n",
      "Iteration: 261, loss: 1.14902758598, reg: 0.0\n",
      "Iteration: 262, loss: 1.22353804111, reg: 0.0\n",
      "Iteration: 263, loss: 1.29767608643, reg: 0.0\n",
      "Iteration: 264, loss: 1.36989855766, reg: 0.0\n",
      "Iteration: 265, loss: 1.19882535934, reg: 0.0\n",
      "Iteration: 266, loss: 1.01955652237, reg: 0.0\n",
      "Iteration: 267, loss: 1.34917008877, reg: 0.0\n",
      "Iteration: 268, loss: 1.11644232273, reg: 0.0\n",
      "Iteration: 269, loss: 1.09592044353, reg: 0.0\n",
      "Iteration: 270, loss: 0.960239291191, reg: 0.0\n",
      "Iteration: 271, loss: 1.47358798981, reg: 0.0\n",
      "Iteration: 272, loss: 1.30281829834, reg: 0.0\n",
      "Iteration: 273, loss: 1.30950832367, reg: 0.0\n",
      "Iteration: 274, loss: 1.1730890274, reg: 0.0\n",
      "Iteration: 275, loss: 1.33927047253, reg: 0.0\n",
      "Iteration: 276, loss: 1.21031570435, reg: 0.0\n",
      "Iteration: 277, loss: 1.29342007637, reg: 0.0\n",
      "Iteration: 278, loss: 1.24177896976, reg: 0.0\n",
      "Iteration: 279, loss: 1.18643021584, reg: 0.0\n",
      "Iteration: 280, loss: 1.21048402786, reg: 0.0\n",
      "Iteration: 281, loss: 1.29288291931, reg: 0.0\n",
      "Iteration: 282, loss: 1.33333158493, reg: 0.0\n",
      "Iteration: 283, loss: 1.13348841667, reg: 0.0\n",
      "Iteration: 284, loss: 1.32389068604, reg: 0.0\n",
      "Iteration: 285, loss: 1.25589799881, reg: 0.0\n",
      "Iteration: 286, loss: 1.03383409977, reg: 0.0\n",
      "Iteration: 287, loss: 1.34051990509, reg: 0.0\n",
      "Iteration: 288, loss: 1.10038113594, reg: 0.0\n",
      "Iteration: 289, loss: 1.12737429142, reg: 0.0\n",
      "Iteration: 290, loss: 1.03171110153, reg: 0.0\n",
      "Iteration: 291, loss: 1.06350445747, reg: 0.0\n",
      "Iteration: 292, loss: 1.10711050034, reg: 0.0\n",
      "Iteration: 293, loss: 1.27270746231, reg: 0.0\n",
      "Iteration: 294, loss: 1.18722105026, reg: 0.0\n",
      "Iteration: 295, loss: 1.33290553093, reg: 0.0\n",
      "Iteration: 296, loss: 1.24512398243, reg: 0.0\n",
      "Iteration: 297, loss: 1.13177418709, reg: 0.0\n",
      "Iteration: 298, loss: 1.44617319107, reg: 0.0\n",
      "Iteration: 299, loss: 1.29956769943, reg: 0.0\n",
      "Iteration: 300, loss: 1.10249638557, reg: 0.0\n",
      "Iteration: 301, loss: 1.36095738411, reg: 0.0\n",
      "Iteration: 302, loss: 1.46476495266, reg: 0.0\n",
      "Iteration: 303, loss: 1.41664981842, reg: 0.0\n",
      "Iteration: 304, loss: 1.40066719055, reg: 0.0\n",
      "Iteration: 305, loss: 1.45378780365, reg: 0.0\n",
      "Iteration: 306, loss: 1.64768815041, reg: 0.0\n",
      "Iteration: 307, loss: 1.63262414932, reg: 0.0\n",
      "Iteration: 308, loss: 1.79275012016, reg: 0.0\n",
      "Iteration: 309, loss: 1.7731821537, reg: 0.0\n",
      "Iteration: 310, loss: 1.87133097649, reg: 0.0\n",
      "Iteration: 311, loss: 1.75145947933, reg: 0.0\n",
      "Iteration: 312, loss: 1.87632775307, reg: 0.0\n",
      "Iteration: 313, loss: 1.70386445522, reg: 0.0\n",
      "Iteration: 314, loss: 1.97195792198, reg: 0.0\n",
      "Iteration: 315, loss: 1.88280010223, reg: 0.0\n",
      "Iteration: 316, loss: 1.4738432169, reg: 0.0\n",
      "Iteration: 317, loss: 1.48576700687, reg: 0.0\n",
      "Iteration: 318, loss: 1.70309305191, reg: 0.0\n",
      "Iteration: 319, loss: 1.6115745306, reg: 0.0\n",
      "Iteration: 320, loss: 1.75019574165, reg: 0.0\n",
      "Iteration: 321, loss: 1.76388013363, reg: 0.0\n",
      "Iteration: 322, loss: 1.89947545528, reg: 0.0\n",
      "Iteration: 323, loss: 1.6048527956, reg: 0.0\n",
      "Iteration: 324, loss: 1.81103920937, reg: 0.0\n",
      "Iteration: 325, loss: 1.66509854794, reg: 0.0\n",
      "Iteration: 326, loss: 1.75753724575, reg: 0.0\n",
      "Iteration: 327, loss: 1.86699163914, reg: 0.0\n",
      "Iteration: 328, loss: 1.5364882946, reg: 0.0\n",
      "Iteration: 329, loss: 1.96906065941, reg: 0.0\n",
      "Iteration: 330, loss: 1.50617349148, reg: 0.0\n",
      "Iteration: 331, loss: 1.86092805862, reg: 0.0\n",
      "Iteration: 332, loss: 1.45743238926, reg: 0.0\n",
      "Iteration: 333, loss: 1.49160647392, reg: 0.0\n",
      "Iteration: 334, loss: 1.5369695425, reg: 0.0\n",
      "Iteration: 335, loss: 1.29572224617, reg: 0.0\n",
      "Iteration: 336, loss: 1.72551655769, reg: 0.0\n",
      "Iteration: 337, loss: 1.42800986767, reg: 0.0\n",
      "Iteration: 338, loss: 1.43780767918, reg: 0.0\n",
      "Iteration: 339, loss: 1.66472887993, reg: 0.0\n",
      "Iteration: 340, loss: 1.2066450119, reg: 0.0\n",
      "Iteration: 341, loss: 1.35002875328, reg: 0.0\n",
      "Iteration: 342, loss: 1.26707160473, reg: 0.0\n",
      "Iteration: 343, loss: 1.3065032959, reg: 0.0\n",
      "Iteration: 344, loss: 1.29500508308, reg: 0.0\n",
      "Iteration: 345, loss: 0.859620332718, reg: 0.0\n",
      "Iteration: 346, loss: 1.22296261787, reg: 0.0\n",
      "Iteration: 347, loss: 0.766002058983, reg: 0.0\n",
      "Iteration: 348, loss: 1.05022978783, reg: 0.0\n",
      "Iteration: 349, loss: 0.950155496597, reg: 0.0\n",
      "Iteration: 350, loss: 0.64482986927, reg: 0.0\n",
      "Iteration: 351, loss: 0.942146778107, reg: 0.0\n",
      "Iteration: 352, loss: 0.762121319771, reg: 0.0\n",
      "Iteration: 353, loss: 0.887140393257, reg: 0.0\n",
      "Iteration: 354, loss: 0.983934164047, reg: 0.0\n",
      "Iteration: 355, loss: 1.02924370766, reg: 0.0\n",
      "Iteration: 356, loss: 0.865290403366, reg: 0.0\n",
      "Iteration: 357, loss: 0.919966459274, reg: 0.0\n",
      "Iteration: 358, loss: 0.981818318367, reg: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 359, loss: 0.92530143261, reg: 0.0\n",
      "Iteration: 360, loss: 1.16173696518, reg: 0.0\n",
      "Iteration: 361, loss: 1.10095453262, reg: 0.0\n",
      "Iteration: 362, loss: 1.31960082054, reg: 0.0\n",
      "Iteration: 363, loss: 1.07620930672, reg: 0.0\n",
      "Iteration: 364, loss: 1.24508273602, reg: 0.0\n",
      "Iteration: 365, loss: 0.905781030655, reg: 0.0\n",
      "Iteration: 366, loss: 0.904014706612, reg: 0.0\n",
      "Iteration: 367, loss: 1.01084160805, reg: 0.0\n",
      "Iteration: 368, loss: 1.32914292812, reg: 0.0\n",
      "Iteration: 369, loss: 0.7894500494, reg: 0.0\n",
      "Iteration: 370, loss: 0.982407212257, reg: 0.0\n",
      "Iteration: 371, loss: 1.21973526478, reg: 0.0\n",
      "Iteration: 372, loss: 0.68829035759, reg: 0.0\n",
      "Iteration: 373, loss: 0.94842672348, reg: 0.0\n",
      "Iteration: 374, loss: 0.853897213936, reg: 0.0\n",
      "Iteration: 375, loss: 0.723544836044, reg: 0.0\n",
      "Iteration: 376, loss: 0.807867288589, reg: 0.0\n",
      "Iteration: 377, loss: 0.753534317017, reg: 0.0\n",
      "Iteration: 378, loss: 0.615457177162, reg: 0.0\n",
      "Iteration: 379, loss: 0.741141438484, reg: 0.0\n",
      "Iteration: 380, loss: 0.288674235344, reg: 0.0\n",
      "Iteration: 381, loss: 0.463797926903, reg: 0.0\n",
      "Iteration: 382, loss: 0.224270582199, reg: 0.0\n",
      "Iteration: 383, loss: 0.493798732758, reg: 0.0\n",
      "Iteration: 384, loss: 0.30994784832, reg: 0.0\n",
      "Iteration: 385, loss: 0.620245337486, reg: 0.0\n",
      "Iteration: 386, loss: 0.487481355667, reg: 0.0\n",
      "Iteration: 387, loss: 0.809388816357, reg: 0.0\n",
      "Iteration: 388, loss: 0.873209238052, reg: 0.0\n",
      "Iteration: 389, loss: 0.733071088791, reg: 0.0\n",
      "Iteration: 390, loss: 0.437476634979, reg: 0.0\n",
      "Iteration: 391, loss: 0.553432106972, reg: 0.0\n",
      "Iteration: 392, loss: 0.59620821476, reg: 0.0\n",
      "Iteration: 393, loss: 0.476120233536, reg: 0.0\n",
      "Iteration: 394, loss: 0.841303825378, reg: 0.0\n",
      "Iteration: 395, loss: 0.805275201797, reg: 0.0\n",
      "Iteration: 396, loss: 0.80741071701, reg: 0.0\n",
      "Iteration: 397, loss: 1.02142119408, reg: 0.0\n",
      "Iteration: 398, loss: 0.776744365692, reg: 0.0\n",
      "Iteration: 399, loss: 0.705153942108, reg: 0.0\n",
      "Iteration: 400, loss: 0.907532930374, reg: 0.0\n",
      "Iteration: 401, loss: 0.766723155975, reg: 0.0\n",
      "Iteration: 402, loss: 0.880272626877, reg: 0.0\n",
      "Iteration: 403, loss: 0.92905163765, reg: 0.0\n",
      "Iteration: 404, loss: 0.90434217453, reg: 0.0\n",
      "Iteration: 405, loss: 0.761980891228, reg: 0.0\n",
      "Iteration: 406, loss: 0.728646278381, reg: 0.0\n",
      "Iteration: 407, loss: 0.934728145599, reg: 0.0\n",
      "Iteration: 408, loss: 0.623644709587, reg: 0.0\n",
      "Iteration: 409, loss: 0.758946537971, reg: 0.0\n",
      "Iteration: 410, loss: 0.595773220062, reg: 0.0\n",
      "Iteration: 411, loss: 0.912624001503, reg: 0.0\n",
      "Iteration: 412, loss: 0.775218367577, reg: 0.0\n",
      "Iteration: 413, loss: 0.832554101944, reg: 0.0\n",
      "Iteration: 414, loss: 0.738203167915, reg: 0.0\n",
      "Iteration: 415, loss: 0.886059522629, reg: 0.0\n",
      "Iteration: 416, loss: 0.67579627037, reg: 0.0\n",
      "Iteration: 417, loss: 0.874199509621, reg: 0.0\n",
      "Iteration: 418, loss: 0.665130972862, reg: 0.0\n",
      "Iteration: 419, loss: 0.731622099876, reg: 0.0\n",
      "Iteration: 420, loss: 0.575742125511, reg: 0.0\n",
      "Iteration: 421, loss: 0.752809882164, reg: 0.0\n",
      "Iteration: 422, loss: 0.650841593742, reg: 0.0\n",
      "Iteration: 423, loss: 0.834166884422, reg: 0.0\n",
      "Iteration: 424, loss: 0.502113938332, reg: 0.0\n",
      "Iteration: 425, loss: 0.687811136246, reg: 0.0\n",
      "Iteration: 426, loss: 0.716097474098, reg: 0.0\n",
      "Iteration: 427, loss: 0.771871328354, reg: 0.0\n",
      "Iteration: 428, loss: 0.538025140762, reg: 0.0\n",
      "Iteration: 429, loss: 0.659004807472, reg: 0.0\n",
      "Iteration: 430, loss: 0.639800190926, reg: 0.0\n",
      "Iteration: 431, loss: 0.764971494675, reg: 0.0\n",
      "Iteration: 432, loss: 0.504525184631, reg: 0.0\n",
      "Iteration: 433, loss: 0.56794989109, reg: 0.0\n",
      "Iteration: 434, loss: 0.524298667908, reg: 0.0\n",
      "Iteration: 435, loss: 0.669143915176, reg: 0.0\n",
      "Iteration: 436, loss: 0.474960923195, reg: 0.0\n",
      "Iteration: 437, loss: 0.441548228264, reg: 0.0\n",
      "Iteration: 438, loss: 0.592352628708, reg: 0.0\n",
      "Iteration: 439, loss: 0.471348166466, reg: 0.0\n",
      "Iteration: 440, loss: 0.512046217918, reg: 0.0\n",
      "Iteration: 441, loss: 0.59998691082, reg: 0.0\n",
      "Iteration: 442, loss: 0.650533437729, reg: 0.0\n",
      "Iteration: 443, loss: 0.791750311852, reg: 0.0\n",
      "Iteration: 444, loss: 0.683055400848, reg: 0.0\n",
      "Iteration: 445, loss: 0.653393030167, reg: 0.0\n",
      "Iteration: 446, loss: 0.827833175659, reg: 0.0\n",
      "Iteration: 447, loss: 1.06587767601, reg: 0.0\n",
      "Iteration: 448, loss: 0.655053734779, reg: 0.0\n",
      "Iteration: 449, loss: 0.590993762016, reg: 0.0\n",
      "Iteration: 450, loss: 0.403733372688, reg: 0.0\n"
     ]
    }
   ],
   "source": [
    "train_wGAN(n_iter=2000, batch_size=50, generator=generator, discriminator=discriminator, regularization = 10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_generated_images(generator, n_images=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([[ 0.48011446],\n",
       "          [ 0.64108396],\n",
       "          [-0.08177859],\n",
       "          [-0.08548088],\n",
       "          [ 0.7578274 ],\n",
       "          [ 0.63911   ],\n",
       "          [ 2.23181   ],\n",
       "          [ 0.5814978 ],\n",
       "          [-0.1326409 ],\n",
       "          [ 0.34209228]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real = get_batch(batch_size=10)\n",
    "discriminator(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([[0.4707209 ],\n",
       "          [0.6744638 ],\n",
       "          [0.43337902],\n",
       "          [0.3057451 ],\n",
       "          [0.41185305],\n",
       "          [0.69154465],\n",
       "          [0.5045203 ],\n",
       "          [0.36493284],\n",
       "          [0.35525772],\n",
       "          [0.35493848]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = get_random_input(10, generator.n_hidden)\n",
    "fake = generator(z)\n",
    "discriminator(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
